{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697db278",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95bf9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost available\n",
      "torch_sparse available\n",
      "Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT SETUP COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.utils import degree, to_undirected, add_self_loops\n",
    "from torch_geometric.transforms import SIGN as SIGN_Transform\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import Node2Vec, GATConv, GCNConv, SAGEConv\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if xgboost is available\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"Installing XGBoost...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
    "    import xgboost as xgb\n",
    "    print(\"XGBoost installed!\")\n",
    "\n",
    "# Check if torch_sparse is available (for fast LP)\n",
    "try:\n",
    "    from torch_sparse import SparseTensor\n",
    "    print(\"torch_sparse available\")\n",
    "except ImportError:\n",
    "    print(\"torch_sparse not found - will use slower LP implementation\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773eda84",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a6e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "Nodes: 19,765\n",
      "Edges: 777,395 (undirected)\n",
      "Labels: 305\n",
      "Train nodes: 5,046\n",
      "Test nodes: 3,365\n",
      "Node features shape: torch.Size([19765, 37])\n",
      "Label sparsity: 0.9696\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "edge_index = torch.load('../data/edge_index.pt')\n",
    "node_features = torch.load('../data/node_features.pt')\n",
    "y = torch.load('../data/y.pt')\n",
    "train_idx = torch.load('../data/train_idx.pt')\n",
    "test_idx = torch.load('../data/test_idx.pt')\n",
    "\n",
    "# Ensure undirected graph\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "num_nodes = node_features.size(0)\n",
    "num_labels = y.size(1)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"DATA LOADED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Nodes: {num_nodes:,}\")\n",
    "print(f\"Edges: {edge_index.size(1)//2:,} (undirected)\")\n",
    "print(f\"Labels: {num_labels}\")\n",
    "print(f\"Train nodes: {len(train_idx):,}\")\n",
    "print(f\"Test nodes: {len(test_idx):,}\")\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "print(f\"Label sparsity: {1 - y[train_idx].float().mean():.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444f29e",
   "metadata": {},
   "source": [
    "## 3. Node2Vec Training (Structural Embeddings)\n",
    "\n",
    "**Configuration:**\n",
    "- Embedding dimension: 64\n",
    "- Walk length: 20\n",
    "- Context size: 10\n",
    "- Walks per node: 10\n",
    "- p=1, q=1 (balanced BFS/DFS)\n",
    "\n",
    "**Strategy:** Check if pre-computed embeddings exist, otherwise train from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0667bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec function defined\n"
     ]
    }
   ],
   "source": [
    "def train_node2vec(edge_index, num_nodes, embedding_dim=64, epochs=100):\n",
    "    \"\"\"\n",
    "    Train Node2Vec embeddings.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        num_nodes: Number of nodes\n",
    "        embedding_dim: Embedding dimension\n",
    "        epochs: Training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Embeddings tensor [num_nodes, embedding_dim]\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING NODE2VEC ({embedding_dim}D)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize Node2Vec\n",
    "    model = Node2Vec(\n",
    "        edge_index=edge_index,\n",
    "        embedding_dim=embedding_dim,\n",
    "        walk_length=20,\n",
    "        context_size=10,\n",
    "        walks_per_node=10,\n",
    "        p=1.0,  # Return parameter\n",
    "        q=1.0,  # In-out parameter\n",
    "        num_negative_samples=1,\n",
    "        sparse=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create data loader (num_workers=0 for Windows)\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Optimizer (sparse optimizer for sparse embeddings)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1), desc=f\"Node2Vec {embedding_dim}d\"):\n",
    "        epoch_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        total_loss += epoch_loss / len(loader)\n",
    "        \n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            avg_loss = epoch_loss / len(loader)\n",
    "            print(f\"  Epoch {epoch:3d} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(torch.arange(num_nodes, device=device))\n",
    "    \n",
    "    print(f\"\\n✓ Node2Vec {embedding_dim}d training complete!\")\n",
    "    print(f\"  Final loss: {total_loss / epochs:.4f}\")\n",
    "    print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return embeddings.cpu()\n",
    "\n",
    "print(\"Node2Vec function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0feda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed Node2Vec embeddings from ../data/node2vec_64d_draft8.pt\n",
      "✓ Loaded embeddings: torch.Size([19765, 64])\n",
      "\n",
      "Node2Vec embeddings ready: torch.Size([19765, 64])\n"
     ]
    }
   ],
   "source": [
    "# Check if pre-computed Node2Vec embeddings exist\n",
    "node2vec_path = '../data/node2vec_64d_draft8.pt'\n",
    "\n",
    "if os.path.exists(node2vec_path):\n",
    "    print(f\"Loading pre-computed Node2Vec embeddings from {node2vec_path}\")\n",
    "    node2vec_emb = torch.load(node2vec_path)\n",
    "    print(f\"✓ Loaded embeddings: {node2vec_emb.shape}\")\n",
    "else:\n",
    "    print(f\"Pre-computed embeddings not found. Training Node2Vec from scratch...\")\n",
    "    node2vec_emb = train_node2vec(edge_index, num_nodes, embedding_dim=64, epochs=100)\n",
    "    \n",
    "    # Save for future use\n",
    "    torch.save(node2vec_emb, node2vec_path)\n",
    "    print(f\"✓ Saved embeddings to {node2vec_path}\")\n",
    "\n",
    "print(f\"\\nNode2Vec embeddings ready: {node2vec_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db7ac1",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering (103 Features)\n",
    "\n",
    "**Components:**\n",
    "- 37 biological features (original)\n",
    "- 64 Node2Vec embeddings (structural)\n",
    "- 1 log-degree (graph statistic)\n",
    "- 1 PageRank (graph centrality)\n",
    "\n",
    "**Total: 103 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14eef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing graph statistics...\n",
      "✓ Log-degree feature: torch.Size([19765, 1])\n",
      "  Mean: 3.38, Max: 8.33\n"
     ]
    }
   ],
   "source": [
    "# Compute log-degree features\n",
    "print(\"Computing graph statistics...\")\n",
    "row, col = edge_index\n",
    "node_deg = degree(row, num_nodes).float()\n",
    "log_degree = torch.log(node_deg + 1).unsqueeze(1)\n",
    "\n",
    "print(f\"✓ Log-degree feature: {log_degree.shape}\")\n",
    "print(f\"  Mean: {log_degree.mean():.2f}, Max: {log_degree.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9989e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PageRank (this may take a few minutes)...\n",
      "✓ PageRank feature: torch.Size([19765, 1])\n",
      "  Mean: 0.000051, Max: 0.002813\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank using NetworkX\n",
    "print(\"\\nComputing PageRank (this may take a few minutes)...\")\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "edges_list = edge_index.t().cpu().numpy().tolist()\n",
    "G.add_edges_from(edges_list)\n",
    "\n",
    "pagerank_dict = nx.pagerank(G, max_iter=100, tol=1e-6)\n",
    "pagerank = torch.tensor([pagerank_dict[i] for i in range(num_nodes)], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"✓ PageRank feature: {pagerank.shape}\")\n",
    "print(f\"  Mean: {pagerank.mean():.6f}, Max: {pagerank.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32f517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling NaN values...\n",
      "✓ NaNs after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle NaN values in node features\n",
    "print(\"\\nHandling NaN values...\")\n",
    "node_features_np = node_features.cpu().numpy()\n",
    "\n",
    "# Replace NaN with median of training nodes\n",
    "train_features = node_features_np[train_idx.cpu().numpy()]\n",
    "medians = np.nanmedian(train_features, axis=0)\n",
    "\n",
    "for i in range(node_features_np.shape[1]):\n",
    "    mask = np.isnan(node_features_np[:, i])\n",
    "    if mask.any():\n",
    "        node_features_np[mask, i] = medians[i]\n",
    "\n",
    "node_features_clean = torch.tensor(node_features_np, dtype=torch.float32)\n",
    "\n",
    "print(f\"✓ NaNs after imputation: {torch.isnan(node_features_clean).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ec582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "================================================================================\n",
      "Enhanced features shape: torch.Size([19765, 103])\n",
      "Breakdown:\n",
      "  Biological features: 37\n",
      "  Node2Vec embeddings: 64\n",
      "  Log-degree: 1\n",
      "  PageRank: 1\n",
      "  Total: 103\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all features\n",
    "# [37 bio] + [64 Node2Vec] + [1 log-degree] + [1 PageRank] = 103 features\n",
    "enhanced_features = torch.cat([\n",
    "    node_features_clean,\n",
    "    node2vec_emb,\n",
    "    log_degree,\n",
    "    pagerank\n",
    "], dim=1)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Enhanced features shape: {enhanced_features.shape}\")\n",
    "print(f\"Breakdown:\")\n",
    "print(f\"  Biological features: 37\")\n",
    "print(f\"  Node2Vec embeddings: 64\")\n",
    "print(f\"  Log-degree: 1\")\n",
    "print(f\"  PageRank: 1\")\n",
    "print(f\"  Total: {enhanced_features.shape[1]}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9723e",
   "metadata": {},
   "source": [
    "## 5. Train/Val Split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4a4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val Split:\n",
      "  Training nodes: 4,036\n",
      "  Validation nodes: 1,010\n",
      "  Test nodes: 3,365\n"
     ]
    }
   ],
   "source": [
    "# Create validation split for proper evaluation\n",
    "train_idx_np = train_idx.cpu().numpy()\n",
    "train_idx_sub, val_idx_sub = train_test_split(\n",
    "    train_idx_np, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_idx_sub = torch.tensor(train_idx_sub, dtype=torch.long)\n",
    "val_idx_sub = torch.tensor(val_idx_sub, dtype=torch.long)\n",
    "\n",
    "# Create masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_idx_sub] = True\n",
    "val_mask[val_idx_sub] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "print(f\"Train/Val Split:\")\n",
    "print(f\"  Training nodes: {len(train_idx_sub):,}\")\n",
    "print(f\"  Validation nodes: {len(val_idx_sub):,}\")\n",
    "print(f\"  Test nodes: {len(test_idx):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa8d5d",
   "metadata": {},
   "source": [
    "## 6. Baseline Models - GNN Experiments\n",
    "\n",
    "**Tested Architectures:**\n",
    "- GCN (Graph Convolutional Network)\n",
    "- GAT (Graph Attention Network)\n",
    "- GraphSAGE (Neighbor Sampling)\n",
    "\n",
    "**Key Finding:** All GNNs performed poorly (AP < 0.052) due to extremely low homophily (0.0252).\n",
    "GNNs rely on message-passing which assumes neighbors share similar labels - not true in this network.\n",
    "\n",
    "Below are the model definitions and a summary of results from our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac430bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Helper function for evaluation\n",
    "def evaluate_ap(y_true, y_pred, mask):\n",
    "    \"\"\"Compute micro-averaged Average Precision.\"\"\"\n",
    "    y_true_np = y_true[mask].cpu().numpy().ravel()\n",
    "    y_pred_np = y_pred[mask].cpu().detach().numpy().ravel()\n",
    "    return average_precision_score(y_true_np, y_pred_np, average='micro')\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6f7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GCN model defined\n"
     ]
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"Graph Convolutional Network\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(GCNConv(in_dim, hidden_dim))\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        self.convs.append(GCNConv(hidden_dim, out_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.convs[-1](x, edge_index)\n",
    "\n",
    "print(\"✓ GCN model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b25bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GraphSAGE model defined\n"
     ]
    }
   ],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"GraphSAGE with neighbor sampling\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        self.convs.append(SAGEConv(hidden_dim, out_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.convs[-1](x, edge_index)\n",
    "\n",
    "print(\"✓ GraphSAGE model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "045c4ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GAT model defined\n"
     ]
    }
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network\n",
    "    \n",
    "    Architecture:\n",
    "    - 2 GAT layers\n",
    "    - 4 attention heads\n",
    "    - 256 hidden channels\n",
    "    - Dropout: 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, \n",
    "                 heads=4, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(\n",
    "            in_channels, \n",
    "            hidden_channels, \n",
    "            heads=heads, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.conv2 = GATConv(\n",
    "            hidden_channels * heads, \n",
    "            hidden_channels, \n",
    "            heads=heads, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_channels * heads, out_channels)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # First GAT layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Classification layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"✓ GAT model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a895e",
   "metadata": {},
   "source": [
    "### GNN Experimental Results Summary\n",
    "\n",
    "We tested all three GNN architectures with various configurations:\n",
    "\n",
    "| Model | Configuration | Validation AP | Kaggle AP |\n",
    "|-------|--------------|---------------|-----------|\n",
    "| GraphSAGE | 3 layers, 256 hidden | 0.024 | 0.026 |\n",
    "| GCN | 3 layers, 256 hidden | 0.032 | 0.036 |\n",
    "| GAT | 4 heads, 256 hidden | 0.048 | 0.052 |\n",
    "\n",
    "**Key Insight:** All GNNs significantly underperformed compared to Label Propagation (0.057). \n",
    "\n",
    "**Reason:** Low homophily (0.0252) means only 2.5% of edges connect nodes with shared diseases. GNNs aggregate neighbor features assuming homophily, making them ineffective for this heterophilic graph.\n",
    "\n",
    "**Conclusion:** Shifted focus to Label Propagation (exploits structure without homophily assumption) and SIGN (pre-computes multi-hop features, avoiding message-passing issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2943b00",
   "metadata": {},
   "source": [
    "## 7. SIGN Model Training\n",
    "\n",
    "**SIGN (Scalable Inception GNN):**\n",
    "- Pre-computes multi-hop aggregations (K=3)\n",
    "- Concatenates [X, AX, A²X, A³X] + label embeddings\n",
    "- Processes with MLP (no runtime message-passing)\n",
    "- Uses Focal Loss for extreme class imbalance\n",
    "- Label reuse: Embeds training labels into 32-dim space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4fb9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Focal Loss initialized (alpha=0.25, gamma=2.0)\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing extreme class imbalance.\n",
    "    \n",
    "    FL(pt) = -alpha * (1 - pt)^gamma * log(pt)\n",
    "    \n",
    "    Args:\n",
    "        alpha: Weighting factor for positive class (0.25 for extreme imbalance)\n",
    "        gamma: Focusing parameter (2.0 recommended)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: Model logits [N, num_labels]\n",
    "            targets: Binary labels [N, num_labels]\n",
    "        \"\"\"\n",
    "        # Binary cross-entropy with logits\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Compute pt (probability of correct class)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        \n",
    "        # Focal loss formula\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "print(\"✓ Focal Loss initialized (alpha=0.25, gamma=2.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5df5c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Label Reuse Encoder: 305 -> 32 dimensions\n"
     ]
    }
   ],
   "source": [
    "class LabelReuseEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode labels into lower-dimensional space for label reuse.\n",
    "    Training labels are projected to label_embed_dim.\n",
    "    Validation/test nodes get zero features to prevent leakage.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels, label_embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.label_encoder = nn.Linear(num_labels, label_embed_dim, bias=False)\n",
    "        self.label_embed_dim = label_embed_dim\n",
    "    \n",
    "    def forward(self, labels, train_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: Full label matrix [num_nodes, num_labels]\n",
    "            train_mask: Boolean mask for training nodes\n",
    "        \n",
    "        Returns:\n",
    "            Label features [num_nodes, label_embed_dim]\n",
    "        \"\"\"\n",
    "        label_features = torch.zeros(labels.size(0), self.label_embed_dim, device=labels.device)\n",
    "        \n",
    "        # Only encode training labels\n",
    "        if train_mask.any():\n",
    "            label_features[train_mask] = self.label_encoder(labels[train_mask])\n",
    "        \n",
    "        return label_features\n",
    "\n",
    "# Initialize label reuse encoder\n",
    "label_encoder = LabelReuseEncoder(num_labels, label_embed_dim=32).to(device)\n",
    "print(f\"✓ Label Reuse Encoder: {num_labels} -> 32 dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79806fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'='*80}\n",
      "Pre-computing SIGN features (K=3)...\n",
      "This may take several minutes...\n",
      "================================================================================\n",
      "\n",
      "Original features: torch.Size([19765, 37])\n",
      "SIGN x0 (self): torch.Size([19765, 37])\n",
      "SIGN x1 (1-hop): torch.Size([19765, 37])\n",
      "SIGN x2 (2-hop): torch.Size([19765, 37])\n",
      "SIGN x3 (3-hop): torch.Size([19765, 37])\n",
      "\n",
      "✓ SIGN pre-computation complete!\n"
     ]
    }
   ],
   "source": [
    "# Create PyG Data object for SIGN transformation\n",
    "data = Data(\n",
    "    x=node_features_clean,\n",
    "    edge_index=edge_index,\n",
    "    y=y\n",
    ")\n",
    "\n",
    "print(\"\\n{'='*80}\")\n",
    "print(\"Pre-computing SIGN features (K=3)...\")\n",
    "print(\"This may take several minutes...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Apply SIGN transform to pre-compute multi-hop features\n",
    "sign_transform = SIGN_Transform(K=3)\n",
    "data_sign = sign_transform(data)\n",
    "\n",
    "print(f\"Original features: {data.x.shape}\")\n",
    "print(f\"SIGN x0 (self): {data_sign.x.shape}\")\n",
    "print(f\"SIGN x1 (1-hop): {data_sign.x1.shape}\")\n",
    "print(f\"SIGN x2 (2-hop): {data_sign.x2.shape}\")\n",
    "print(f\"SIGN x3 (3-hop): {data_sign.x3.shape}\")\n",
    "\n",
    "# Store SIGN features\n",
    "sign_features = {\n",
    "    'x0': data_sign.x,\n",
    "    'x1': data_sign.x1,\n",
    "    'x2': data_sign.x2,\n",
    "    'x3': data_sign.x3\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ SIGN pre-computation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c0f9ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SIGN Model initialized\n",
      "  Parameters: 513,841\n"
     ]
    }
   ],
   "source": [
    "class SIGN_MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    SIGN model: Concatenate multi-hop features and process with MLP.\n",
    "    \n",
    "    Architecture:\n",
    "    - Concatenate [X, AX, A²X, A³X, label_features]\n",
    "    - 3-layer MLP with dropout\n",
    "    - Output: logits for 305 labels\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, label_embed_dim, hidden_channels, out_channels, K=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        \n",
    "        # Total input dimension: (K+1) * in_channels + label_embed_dim\n",
    "        total_in_dim = (K + 1) * in_channels + label_embed_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(total_in_dim, hidden_channels),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sign_features, label_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sign_features: Dict with keys 'x0', 'x1', 'x2', 'x3'\n",
    "            label_features: Encoded label features [num_nodes, label_embed_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Logits [num_nodes, out_channels]\n",
    "        \"\"\"\n",
    "        # Concatenate all SIGN features\n",
    "        xs = [sign_features[f'x{i}'] for i in range(self.K + 1)]\n",
    "        x = torch.cat(xs + [label_features], dim=1)\n",
    "        \n",
    "        return self.mlp(x)\n",
    "\n",
    "# Initialize SIGN model\n",
    "sign_model = SIGN_MLP(\n",
    "    in_channels=37,  # Original feature dimension\n",
    "    label_embed_dim=32,\n",
    "    hidden_channels=512,\n",
    "    out_channels=305,\n",
    "    K=3,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"✓ SIGN Model initialized\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in sign_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2192aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'='*80}\n",
      "Training SIGN Model with Label Reuse + Focal Loss\n",
      "================================================================================\n",
      "\n",
      "Epoch  10 | Loss: 0.0131 | Val AP: 0.032704\n",
      "Epoch  20 | Loss: 0.0105 | Val AP: 0.056600\n",
      "Epoch  30 | Loss: 0.0100 | Val AP: 0.058438\n",
      "Epoch  40 | Loss: 0.0100 | Val AP: 0.054440\n",
      "Epoch  50 | Loss: 0.0099 | Val AP: 0.052905\n",
      "Epoch  60 | Loss: 0.0098 | Val AP: 0.052227\n",
      "Epoch  70 | Loss: 0.0097 | Val AP: 0.048389\n",
      "Epoch  80 | Loss: 0.0096 | Val AP: 0.054844\n",
      "Epoch  90 | Loss: 0.0096 | Val AP: 0.056686\n",
      "Epoch 100 | Loss: 0.0096 | Val AP: 0.060097\n",
      "Epoch 110 | Loss: 0.0096 | Val AP: 0.059163\n",
      "Epoch 120 | Loss: 0.0095 | Val AP: 0.060197\n",
      "Epoch 130 | Loss: 0.0095 | Val AP: 0.057753\n",
      "Epoch 140 | Loss: 0.0096 | Val AP: 0.062166\n",
      "Epoch 150 | Loss: 0.0096 | Val AP: 0.054324\n",
      "Epoch 160 | Loss: 0.0096 | Val AP: 0.062942\n",
      "Epoch 170 | Loss: 0.0096 | Val AP: 0.060381\n",
      "Epoch 180 | Loss: 0.0096 | Val AP: 0.062777\n",
      "Epoch 190 | Loss: 0.0096 | Val AP: 0.059483\n",
      "Epoch 200 | Loss: 0.0096 | Val AP: 0.060180\n",
      "\n",
      "✓ SIGN Training Complete!\n",
      "✓ Best Validation AP: 0.062942\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Move SIGN features to device\n",
    "sign_features_device = {k: v.to(device) for k, v in sign_features.items()}\n",
    "y_device = y.to(device).float()\n",
    "train_idx_sub_device = train_idx_sub.to(device)\n",
    "val_idx_sub_device = val_idx_sub.to(device)\n",
    "test_idx_device = test_idx.to(device)\n",
    "\n",
    "# Initialize optimizer and loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(sign_model.parameters()) + list(label_encoder.parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "best_val_ap = 0\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n{'='*80}\")\n",
    "print(\"Training SIGN Model with Label Reuse + Focal Loss\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    sign_model.train()\n",
    "    label_encoder.train()\n",
    "    \n",
    "    # Create train mask for label reuse (prevent leakage)\n",
    "    train_mask_device = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "    train_mask_device[train_idx_sub_device] = True\n",
    "    \n",
    "    # 50% random masking during training to prevent overfitting\n",
    "    if epoch % 2 == 0:  # Mask every other epoch\n",
    "        mask_ratio = 0.5\n",
    "        num_to_mask = int(len(train_idx_sub_device) * mask_ratio)\n",
    "        mask_indices = torch.randperm(len(train_idx_sub_device), device=device)[:num_to_mask]\n",
    "        train_mask_device[train_idx_sub_device[mask_indices]] = False\n",
    "    \n",
    "    # Encode labels\n",
    "    label_features = label_encoder(y_device, train_mask_device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    out = sign_model(sign_features_device, label_features)\n",
    "    \n",
    "    # Compute loss on training nodes\n",
    "    loss = criterion(out[train_idx_sub_device], y_device[train_idx_sub_device])\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation phase (every 10 epochs)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        sign_model.eval()\n",
    "        label_encoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Use only training labels for validation\n",
    "            train_mask_val = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "            train_mask_val[train_idx_sub_device] = True\n",
    "            \n",
    "            label_features_val = label_encoder(y_device, train_mask_val)\n",
    "            out_val = sign_model(sign_features_device, label_features_val)\n",
    "            \n",
    "            # Compute validation AP\n",
    "            val_probs = torch.sigmoid(out_val[val_idx_sub_device])\n",
    "            val_labels = y_device[val_idx_sub_device]\n",
    "            \n",
    "            val_ap = average_precision_score(\n",
    "                val_labels.cpu().numpy().ravel(),\n",
    "                val_probs.cpu().numpy().ravel(),\n",
    "                average='micro'\n",
    "            )\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Val AP: {val_ap:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_ap > best_val_ap:\n",
    "                best_val_ap = val_ap\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'sign_model': sign_model.state_dict(),\n",
    "                    'label_encoder': label_encoder.state_dict(),\n",
    "                    'val_ap': val_ap\n",
    "                }, 'best_sign_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "print(f\"\\n✓ SIGN Training Complete!\")\n",
    "print(f\"✓ Best Validation AP: {best_val_ap:.6f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660285b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model with Val AP: 0.062942\n",
      "\n",
      "SIGN Predictions Generated:\n",
      "  Train shape: (5046, 305)\n",
      "  Val shape: (1010, 305)\n",
      "  Test shape: (3365, 305)\n",
      "  Test mean: 0.257481\n",
      "  Test range: [0.1133, 0.4081]\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_sign_model.pt')\n",
    "sign_model.load_state_dict(checkpoint['sign_model'])\n",
    "label_encoder.load_state_dict(checkpoint['label_encoder'])\n",
    "\n",
    "print(f\"Loaded best model with Val AP: {checkpoint['val_ap']:.6f}\")\n",
    "\n",
    "# Generate predictions for full training set (for stacking)\n",
    "sign_model.eval()\n",
    "label_encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use ALL training labels (not just subset)\n",
    "    full_train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "    full_train_mask[train_idx.to(device)] = True\n",
    "    \n",
    "    label_features_full = label_encoder(y_device, full_train_mask)\n",
    "    sign_out = sign_model(sign_features_device, label_features_full)\n",
    "    sign_probs = torch.sigmoid(sign_out)\n",
    "\n",
    "# Extract predictions\n",
    "sign_train_preds = sign_probs[train_idx.to(device)].cpu().numpy()\n",
    "sign_val_preds = sign_probs[val_idx_sub_device].cpu().numpy()\n",
    "sign_test_preds = sign_probs[test_idx_device].cpu().numpy()\n",
    "\n",
    "print(f\"\\nSIGN Predictions Generated:\")\n",
    "print(f\"  Train shape: {sign_train_preds.shape}\")\n",
    "print(f\"  Val shape: {sign_val_preds.shape}\")\n",
    "print(f\"  Test shape: {sign_test_preds.shape}\")\n",
    "print(f\"  Test mean: {sign_test_preds.mean():.6f}\")\n",
    "print(f\"  Test range: [{sign_test_preds.min():.4f}, {sign_test_preds.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153d42c",
   "metadata": {},
   "source": [
    "## 8. XGBoost Training (305 Binary Classifiers)\n",
    "\n",
    "**Configuration:**\n",
    "- One XGBoost classifier per disease label\n",
    "- Class weighting via `scale_pos_weight` (handles imbalance)\n",
    "- Features: All 103 enhanced features\n",
    "- Hyperparameters: n_estimators=100, max_depth=5, lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628c649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Training Data:\n",
      "  Features: (5046, 103)\n",
      "  Labels: (5046, 305)\n",
      "  Test Features: (3365, 103)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for XGBoost\n",
    "enhanced_features_np = enhanced_features.cpu().numpy()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "enhanced_features_scaled = scaler.fit_transform(enhanced_features_np)\n",
    "\n",
    "X_train = enhanced_features_scaled[train_idx.cpu().numpy()]\n",
    "y_train_np = y[train_idx].cpu().numpy()\n",
    "X_test = enhanced_features_scaled[test_idx.cpu().numpy()]\n",
    "\n",
    "print(f\"XGBoost Training Data:\")\n",
    "print(f\"  Features: {X_train.shape}\")\n",
    "print(f\"  Labels: {y_train_np.shape}\")\n",
    "print(f\"  Test Features: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff196c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training XGBoost Models (305 labels)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df9a0ad68084534b3232f4822980057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training XGBoost:   0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ XGBoost Training Complete!\n",
      "  Test Predictions: (3365, 305)\n",
      "  Mean: 0.067144\n",
      "  Range: [0.0000, 0.9989]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train 305 binary XGBoost classifiers (one per label)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Training XGBoost Models (305 labels)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "xgb_models = []\n",
    "xgb_test_preds = np.zeros((len(test_idx), num_labels))\n",
    "\n",
    "for label_idx in tqdm(range(num_labels), desc=\"Training XGBoost\"):\n",
    "    y_label = y_train_np[:, label_idx]\n",
    "    \n",
    "    # Calculate pos_weight for imbalance\n",
    "    num_pos = y_label.sum()\n",
    "    num_neg = len(y_label) - num_pos\n",
    "    scale_pos_weight = num_neg / max(num_pos, 1)\n",
    "    \n",
    "    # Configure XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method='hist',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    xgb_model.fit(X_train, y_label)\n",
    "    \n",
    "    # Predict\n",
    "    xgb_test_preds[:, label_idx] = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_models.append(xgb_model)\n",
    "\n",
    "print(f\"\\n✓ XGBoost Training Complete!\")\n",
    "print(f\"  Test Predictions: {xgb_test_preds.shape}\")\n",
    "print(f\"  Mean: {xgb_test_preds.mean():.6f}\")\n",
    "print(f\"  Range: [{xgb_test_preds.min():.4f}, {xgb_test_preds.max():.4f}]\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df1c2f",
   "metadata": {},
   "source": [
    "## 9. Label Propagation\n",
    "\n",
    "**Algorithm:** Iterative label smoothing on graph structure  \n",
    "**Formula:** Y^(t+1) = α · Â · Y^(t) + (1-α) · Y^(0)  \n",
    "where Â is the symmetrically normalized adjacency matrix.\n",
    "\n",
    "**Configuration:**\n",
    "- Alpha: 0.85 (balance between smoothing and anchoring)\n",
    "- Iterations: 50 (converges in ~20-30 typically)\n",
    "- Normalization: Symmetric (D^(-1/2) A D^(-1/2))\n",
    "\n",
    "**Key Advantage:** Works well on heterophilic graphs (low homophily) because it doesn't assume neighbor similarity - just smooths via random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f8cb721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Label Propagation function defined\n"
     ]
    }
   ],
   "source": [
    "def label_propagation(\n",
    "    edge_index: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    train_mask: torch.Tensor,\n",
    "    num_iterations: int = 50,\n",
    "    alpha: float = 0.85,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Label propagation using sparse matrix operations.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: Graph edges [2, E]\n",
    "        y: Labels [N, C]\n",
    "        train_mask: Boolean mask for training nodes\n",
    "        num_iterations: Number of propagation iterations\n",
    "        alpha: Propagation weight (0.85 = 85% neighbor influence)\n",
    "        \n",
    "    Returns:\n",
    "        Propagated soft labels [N, C]\n",
    "    \"\"\"\n",
    "    num_nodes = y.shape[0]\n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    # Add self-loops\n",
    "    edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "    \n",
    "    # Symmetric normalization: D^(-1/2) A D^(-1/2)\n",
    "    row, col = edge_index_with_loops\n",
    "    deg = degree(col, num_nodes=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    \n",
    "    # Compute normalized adjacency weights\n",
    "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    \n",
    "    # Create sparse adjacency matrix\n",
    "    try:\n",
    "        from torch_sparse import SparseTensor\n",
    "        adj = SparseTensor(row=col, col=row, value=norm, \n",
    "                          sparse_sizes=(num_nodes, num_nodes))\n",
    "        use_torch_sparse = True\n",
    "    except ImportError:\n",
    "        # Fallback to PyTorch sparse tensors\n",
    "        indices = torch.stack([row, col], dim=0)\n",
    "        values = norm\n",
    "        adj = torch.sparse_coo_tensor(indices, values, (num_nodes, num_nodes))\n",
    "        use_torch_sparse = False\n",
    "    \n",
    "    # Initialize labels\n",
    "    Y = torch.zeros(num_nodes, num_classes, dtype=torch.float32)\n",
    "    Y[train_mask] = y[train_mask].float()\n",
    "    Y_init = Y.clone()\n",
    "    \n",
    "    # Propagation loop\n",
    "    print(f\"\\nRunning Label Propagation (alpha={alpha}, iterations={num_iterations})...\")\n",
    "    for iteration in tqdm(range(num_iterations), desc=\"LP\"):\n",
    "        if use_torch_sparse:\n",
    "            Y = alpha * (adj @ Y) + (1 - alpha) * Y_init\n",
    "        else:\n",
    "            Y = alpha * torch.sparse.mm(adj, Y) + (1 - alpha) * Y_init\n",
    "        \n",
    "        # Re-anchor training labels\n",
    "        Y[train_mask] = Y_init[train_mask]\n",
    "    \n",
    "    print(f\"✓ Label Propagation Complete!\")\n",
    "    \n",
    "    return Y\n",
    "\n",
    "print(\"✓ Label Propagation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c957cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Label Propagation (alpha=0.85, iterations=50)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446b839dbf1b4e0ba951b8d42cbb1767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LP:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Label Propagation Complete!\n",
      "\n",
      "Label Propagation Test Predictions:\n",
      "  Shape: (3365, 305)\n",
      "  Mean: 0.009248\n",
      "  Range: [0.0000, 0.3152]\n"
     ]
    }
   ],
   "source": [
    "# Run Label Propagation\n",
    "train_mask_full = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask_full[train_idx] = True\n",
    "\n",
    "lp_predictions = label_propagation(\n",
    "    edge_index=edge_index,\n",
    "    y=y,\n",
    "    train_mask=train_mask_full,\n",
    "    num_iterations=50,\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "# Extract test predictions\n",
    "lp_test_preds = lp_predictions[test_idx].cpu().numpy()\n",
    "\n",
    "print(f\"\\nLabel Propagation Test Predictions:\")\n",
    "print(f\"  Shape: {lp_test_preds.shape}\")\n",
    "print(f\"  Mean: {lp_test_preds.mean():.6f}\")\n",
    "print(f\"  Range: [{lp_test_preds.min():.4f}, {lp_test_preds.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0a013",
   "metadata": {},
   "source": [
    "## 10. Calibration - Fixing Miscalibrated Predictions\n",
    "\n",
    "**Problem:** SIGN and XGBoost produce miscalibrated probabilities (wrong scale).\n",
    "\n",
    "**Solutions:**\n",
    "1. **SIGN:** Temperature scaling - Grid search optimal T that maximizes validation AP\n",
    "2. **XGBoost:** Simple rescaling to match target label density (~0.012)\n",
    "\n",
    "**Goal:** Match prediction distribution to training label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0b3257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CALIBRATION - FIXING MISCALIBRATED PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Target label density: 0.030357\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import logit, expit\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CALIBRATION - FIXING MISCALIBRATED PREDICTIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Target label density (from training set)\n",
    "target_mean = y[train_idx].float().mean().item()\n",
    "print(f\"\\nTarget label density: {target_mean:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b92a9e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] SIGN Temperature Scaling...\n",
      "  Current SIGN mean: 0.257481\n",
      "\n",
      "  Searching for optimal temperature...\n",
      "    T=  0.5 | Val AP: 0.062942 | Mean: 0.100968\n",
      "    T=  0.8 | Val AP: 0.062942 | Mean: 0.199113\n",
      "    T=  1.0 | Val AP: 0.062942 | Mean: 0.246131\n",
      "    T=  1.5 | Val AP: 0.062942 | Mean: 0.320702\n",
      "    T=  2.0 | Val AP: 0.062942 | Mean: 0.362590\n",
      "    T=  3.0 | Val AP: 0.062942 | Mean: 0.406923\n",
      "    T=  5.0 | Val AP: 0.062942 | Mean: 0.443687\n",
      "    T=  8.0 | Val AP: 0.062941 | Mean: 0.464703\n",
      "    T= 10.0 | Val AP: 0.062941 | Mean: 0.471743\n",
      "    T= 15.0 | Val AP: 0.062941 | Mean: 0.481150\n",
      "\n",
      "  ✅ Best Temperature: 2.0 (Val AP: 0.062942)\n",
      "\n",
      "  SIGN After Calibration:\n",
      "    Mean: 0.369764\n",
      "    Range: [0.2634, 0.4537]\n"
     ]
    }
   ],
   "source": [
    "# ===== 1. SIGN Temperature Scaling =====\n",
    "print(f\"\\n[1/2] SIGN Temperature Scaling...\")\n",
    "print(f\"  Current SIGN mean: {sign_test_preds.mean():.6f}\")\n",
    "\n",
    "# Convert SIGN probs to logits (with clipping to avoid infinity)\n",
    "sign_test_logits = logit(np.clip(sign_test_preds, 1e-7, 1-1e-7))\n",
    "sign_val_logits = logit(np.clip(sign_val_preds, 1e-7, 1-1e-7))\n",
    "\n",
    "y_val_np = y[val_idx_sub].cpu().numpy()\n",
    "\n",
    "# Grid search for best temperature\n",
    "print(\"\\n  Searching for optimal temperature...\")\n",
    "best_temp = 1.0\n",
    "best_val_ap_calibrated = 0\n",
    "temperature_results = []\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5, 2.0, 3.0, 5.0, 8.0, 10.0, 15.0]:\n",
    "    # Apply temperature scaling: sigmoid(logit / T)\n",
    "    sign_val_calibrated = expit(sign_val_logits / temp)\n",
    "    \n",
    "    # Compute validation AP\n",
    "    val_ap_temp = average_precision_score(\n",
    "        y_val_np.ravel(),\n",
    "        sign_val_calibrated.ravel(),\n",
    "        average='micro'\n",
    "    )\n",
    "    \n",
    "    mean_pred = sign_val_calibrated.mean()\n",
    "    temperature_results.append((temp, val_ap_temp, mean_pred))\n",
    "    \n",
    "    print(f\"    T={temp:5.1f} | Val AP: {val_ap_temp:.6f} | Mean: {mean_pred:.6f}\")\n",
    "    \n",
    "    if val_ap_temp > best_val_ap_calibrated:\n",
    "        best_val_ap_calibrated = val_ap_temp\n",
    "        best_temp = temp\n",
    "\n",
    "print(f\"\\n  ✅ Best Temperature: {best_temp} (Val AP: {best_val_ap_calibrated:.6f})\")\n",
    "\n",
    "# Apply best temperature to test set\n",
    "sign_test_calibrated = expit(sign_test_logits / best_temp)\n",
    "print(f\"\\n  SIGN After Calibration:\")\n",
    "print(f\"    Mean: {sign_test_calibrated.mean():.6f}\")\n",
    "print(f\"    Range: [{sign_test_calibrated.min():.4f}, {sign_test_calibrated.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5420e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/2] XGBoost Rescaling...\n",
      "  Current XGBoost mean: 0.067144\n",
      "\n",
      "  XGBoost After Calibration:\n",
      "    Scale factor: 0.4521\n",
      "    Mean: 0.030357\n",
      "    Range: [0.0000, 0.4516]\n",
      "\n",
      "================================================================================\n",
      "✓ Calibration Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. XGBoost Rescaling =====\n",
    "print(f\"\\n[2/2] XGBoost Rescaling...\")\n",
    "print(f\"  Current XGBoost mean: {xgb_test_preds.mean():.6f}\")\n",
    "\n",
    "# Calculate scale factor to match target distribution\n",
    "xgb_scale_factor = target_mean / xgb_test_preds.mean()\n",
    "\n",
    "xgb_test_calibrated = np.clip(xgb_test_preds * xgb_scale_factor, 0, 1)\n",
    "\n",
    "print(f\"\\n  XGBoost After Calibration:\")\n",
    "print(f\"    Scale factor: {xgb_scale_factor:.4f}\")\n",
    "print(f\"    Mean: {xgb_test_calibrated.mean():.6f}\")\n",
    "print(f\"    Range: [{xgb_test_calibrated.min():.4f}, {xgb_test_calibrated.max():.4f}]\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Calibration Complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a82f76",
   "metadata": {},
   "source": [
    "## 11. Final Ensemble (20% SIGN + 20% XGBoost + 60% LP)\n",
    "\n",
    "**Ensemble Strategy:**\n",
    "- **60% Label Propagation:** Strongest standalone model (AP ~0.057)\n",
    "- **20% SIGN:** Captures multi-hop patterns with label reuse\n",
    "- **20% XGBoost:** Exploits tabular features (Node2Vec, PageRank)\n",
    "\n",
    "**Rationale:** LP is the hero - contributes 95.8% of final performance. SIGN and XGBoost provide marginal refinement (+0.34% improvement).\n",
    "\n",
    "**Final Score:** 0.057353 AP (2nd Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dd1f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL ENSEMBLE PREDICTIONS\n",
      "================================================================================\n",
      "Shape: (3365, 305)\n",
      "Mean: 0.085573\n",
      "Range: [0.0528, 0.3322]\n",
      "\n",
      "Ensemble Weights:\n",
      "  SIGN (calibrated):    20%\n",
      "  XGBoost (calibrated): 20%\n",
      "  Label Propagation:    60%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Ensemble: 20% SIGN + 20% XGBoost + 60% LP\n",
    "ensemble_predictions = (\n",
    "    0.2 * sign_test_calibrated +\n",
    "    0.2 * xgb_test_calibrated +\n",
    "    0.6 * lp_test_preds\n",
    ")\n",
    "\n",
    "# Clip to valid range\n",
    "ensemble_predictions = np.clip(ensemble_predictions, 0.0, 1.0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL ENSEMBLE PREDICTIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Shape: {ensemble_predictions.shape}\")\n",
    "print(f\"Mean: {ensemble_predictions.mean():.6f}\")\n",
    "print(f\"Range: [{ensemble_predictions.min():.4f}, {ensemble_predictions.max():.4f}]\")\n",
    "print(f\"\\nEnsemble Weights:\")\n",
    "print(f\"  SIGN (calibrated):    20%\")\n",
    "print(f\"  XGBoost (calibrated): 20%\")\n",
    "print(f\"  Label Propagation:    60%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98da52",
   "metadata": {},
   "source": [
    "## 12. Generate Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31c455a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Submission function defined\n"
     ]
    }
   ],
   "source": [
    "def create_submission(predictions, filename, test_idx_tensor):\n",
    "    \"\"\"Create submission CSV with correct format.\"\"\"\n",
    "    test_idx_cpu = test_idx_tensor.cpu().numpy()\n",
    "    \n",
    "    # Clip to [0, 1]\n",
    "    predictions = np.clip(predictions, 0.0, 1.0)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    label_columns = [f'label_{i}' for i in range(predictions.shape[1])]\n",
    "    submission = pd.DataFrame(predictions, columns=label_columns)\n",
    "    submission.insert(0, 'node_id', test_idx_cpu)\n",
    "    \n",
    "    # Save\n",
    "    output_path = f'../Submissions/{filename}'\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Saved: {output_path}\")\n",
    "    print(f\"   Shape: {submission.shape}\")\n",
    "    print(f\"   Mean: {predictions.mean():.6f}\")\n",
    "    print(f\"   Range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "print(\"✓ Submission function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f96a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved: ../Submissions/final_submission_ensemble.csv\n",
      "   Shape: (3365, 306)\n",
      "   Mean: 0.085573\n",
      "   Range: [0.0528, 0.3322]\n",
      "\n",
      "================================================================================\n",
      "FINAL SUBMISSION GENERATED\n",
      "================================================================================\n",
      "Expected Kaggle Score: ~0.057353 AP (2nd Place)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create final submission\n",
    "final_submission = create_submission(\n",
    "    ensemble_predictions,\n",
    "    'final_submission_ensemble.csv',\n",
    "    test_idx\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL SUBMISSION GENERATED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Expected Kaggle Score: ~0.057353 AP (2nd Place)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885bf58",
   "metadata": {},
   "source": [
    "## 13. Results Summary & Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0c18e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE - RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Model Performance Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                            Validation AP       Kaggle AP\n",
      "--------------------------------------------------------------------------------\n",
      "GraphSAGE (baseline)                    ~0.024           0.026\n",
      "GCN                                     ~0.032           0.036\n",
      "GAT                                     ~0.048           0.052\n",
      "SIGN (calibrated)                       0.0629          ~0.037\n",
      "XGBoost (103 features)                     N/A          ~0.040\n",
      "Label Propagation (pure)                   N/A        0.057156\n",
      "Ensemble (20-20-60)                        N/A        0.057353\n",
      "--------------------------------------------------------------------------------\n",
      "Final Ranking                                        2nd Place\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Key Insights:\n",
      "1. Low homophily (0.0252) explains why GNNs failed\n",
      "2. Label Propagation carries 95.8% of final performance\n",
      "3. SIGN + XGBoost provide marginal refinement (+0.34%)\n",
      "4. Calibration is critical for ensemble performance\n",
      "5. Feature engineering (Node2Vec + PageRank) helps tabular models\n",
      "\n",
      "================================================================================\n",
      "All outputs saved to ../Submissions/\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PIPELINE COMPLETE - RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<30} {'Validation AP':>15} {'Kaggle AP':>15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'GraphSAGE (baseline)':<30} {'~0.024':>15} {'0.026':>15}\")\n",
    "print(f\"{'GCN':<30} {'~0.032':>15} {'0.036':>15}\")\n",
    "print(f\"{'GAT':<30} {'~0.048':>15} {'0.052':>15}\")\n",
    "print(f\"{'SIGN (calibrated)':<30} {f'{best_val_ap:.4f}':>15} {'~0.037':>15}\")\n",
    "print(f\"{'XGBoost (103 features)':<30} {'N/A':>15} {'~0.040':>15}\")\n",
    "print(f\"{'Label Propagation (pure)':<30} {'N/A':>15} {'0.057156':>15}\")\n",
    "print(f\"{'Ensemble (20-20-60)':<30} {'N/A':>15} {'0.057353':>15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Final Ranking':<30} {' ':>15} {'2nd Place':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\\nKey Insights:\")\n",
    "print(\"1. Low homophily (0.0252) explains why GNNs failed\")\n",
    "print(\"2. Label Propagation carries 95.8% of final performance\")\n",
    "print(\"3. SIGN + XGBoost provide marginal refinement (+0.34%)\")\n",
    "print(\"4. Calibration is critical for ensemble performance\")\n",
    "print(\"5. Feature engineering (Node2Vec + PageRank) helps tabular models\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All outputs saved to ../Submissions/\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48197cf7",
   "metadata": {},
   "source": [
    "## End of Pipeline\n",
    "\n",
    "**Next Steps:**\n",
    "1. Submit `final_submission_ensemble.csv` to Kaggle\n",
    "2. Experiment with multi-scale LP (multiple α values)\n",
    "3. Try learnable ensemble weights (meta-learning)\n",
    "4. Explore per-label thresholding for precision-recall optimization\n",
    "\n",
    "**Reproducibility Notes:**\n",
    "- Set SEED=42 for deterministic results\n",
    "- Node2Vec embeddings saved to `../data/node2vec_64d_draft8.pt`\n",
    "- SIGN model checkpoint saved to `best_sign_model.pt`\n",
    "- All submissions saved to `../Submissions/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv - GNN Project)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
