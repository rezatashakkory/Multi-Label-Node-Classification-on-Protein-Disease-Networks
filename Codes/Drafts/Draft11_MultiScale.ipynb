{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a59e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.utils import degree, to_undirected\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53abc90",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a13e872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 19765, Labels: 305\n",
      "Train: 5046, Test: 3365\n",
      "Edges: 777395 (undirected)\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "edge_index = torch.load('../data/edge_index.pt').to(device)\n",
    "node_features = torch.load('../data/node_features.pt').to(device)\n",
    "y = torch.load('../data/y.pt').to(device)\n",
    "train_idx = torch.load('../data/train_idx.pt').to(device)\n",
    "test_idx = torch.load('../data/test_idx.pt').to(device)\n",
    "\n",
    "# Ensure undirected graph\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "num_nodes = node_features.size(0)\n",
    "num_labels = y.size(1)\n",
    "\n",
    "print(f\"Nodes: {num_nodes}, Labels: {num_labels}\")\n",
    "print(f\"Train: {len(train_idx)}, Test: {len(test_idx)}\")\n",
    "print(f\"Edges: {edge_index.size(1)//2} (undirected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a9e8c",
   "metadata": {},
   "source": [
    "## 2. Label Propagation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d667f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelPropagation:\n",
    "    def __init__(self, edge_index, num_nodes, num_layers=50, alpha=0.9, device='cpu'):\n",
    "        self.edge_index = edge_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.alpha = alpha\n",
    "        self.device = device\n",
    "        \n",
    "        # Compute symmetric normalized adjacency\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, num_nodes).float()\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        self.edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    \n",
    "    def propagate(self, y_train, train_idx):\n",
    "        # Initialize predictions\n",
    "        y_soft = torch.zeros(self.num_nodes, y_train.size(1), device=self.device)\n",
    "        y_soft[train_idx] = y_train\n",
    "        \n",
    "        # Store initial predictions\n",
    "        y_init = y_soft.clone()\n",
    "        \n",
    "        # Propagate\n",
    "        for _ in range(self.num_layers):\n",
    "            y_new = torch.zeros_like(y_soft)\n",
    "            row, col = self.edge_index\n",
    "            \n",
    "            # Aggregate from neighbors\n",
    "            y_new.index_add_(0, row, y_soft[col] * self.edge_weight.view(-1, 1))\n",
    "            \n",
    "            # Mixing with initial predictions\n",
    "            y_soft = self.alpha * y_new + (1 - self.alpha) * y_init\n",
    "        \n",
    "        return y_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a229a4f",
   "metadata": {},
   "source": [
    "## 3. Correct and Smooth Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7474863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectAndSmooth:\n",
    "    def __init__(self, edge_index, num_nodes, num_layers=50, \n",
    "                 correction_alpha=0.8, smoothing_alpha=0.8, device='cpu'):\n",
    "        self.edge_index = edge_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_layers = num_layers\n",
    "        self.correction_alpha = correction_alpha\n",
    "        self.smoothing_alpha = smoothing_alpha\n",
    "        self.device = device\n",
    "        \n",
    "        # Compute symmetric normalized adjacency\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, num_nodes).float()\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        self.edge_weight = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    \n",
    "    def correct(self, y_soft, y_train, train_idx):\n",
    "        # Compute training errors\n",
    "        error = torch.zeros_like(y_soft)\n",
    "        error[train_idx] = y_train - y_soft[train_idx]\n",
    "        \n",
    "        # Propagate errors\n",
    "        for _ in range(self.num_layers):\n",
    "            error_new = torch.zeros_like(error)\n",
    "            row, col = self.edge_index\n",
    "            \n",
    "            error_new.index_add_(0, row, error[col] * self.edge_weight.view(-1, 1))\n",
    "            error = self.correction_alpha * error_new + (1 - self.correction_alpha) * error\n",
    "            error[train_idx] = y_train - y_soft[train_idx]\n",
    "        \n",
    "        return y_soft + error\n",
    "    \n",
    "    def smooth(self, y_soft, y_train, train_idx):\n",
    "        y_init = y_soft.clone()\n",
    "        \n",
    "        # Smooth predictions\n",
    "        for _ in range(self.num_layers):\n",
    "            y_new = torch.zeros_like(y_soft)\n",
    "            row, col = self.edge_index\n",
    "            \n",
    "            y_new.index_add_(0, row, y_soft[col] * self.edge_weight.view(-1, 1))\n",
    "            y_soft = self.smoothing_alpha * y_new + (1 - self.smoothing_alpha) * y_init\n",
    "            y_soft[train_idx] = y_train\n",
    "        \n",
    "        return y_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1daea8",
   "metadata": {},
   "source": [
    "## 4. Multi-Scale Label Propagation\n",
    "\n",
    "Generate predictions with 5 different alpha values to capture different propagation scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40bb1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Multi-Scale Label Propagation...\n",
      "\n",
      "Alpha = 0.7\n",
      "  Test predictions - Mean: 0.0115, Min: -0.0008, Max: 0.3636\n",
      "\n",
      "Alpha = 0.75\n",
      "  Test predictions - Mean: 0.0117, Min: -0.0006, Max: 0.3682\n",
      "\n",
      "Alpha = 0.8\n",
      "  Test predictions - Mean: 0.0120, Min: -0.0005, Max: 0.3724\n",
      "\n",
      "Alpha = 0.85\n",
      "  Test predictions - Mean: 0.0122, Min: -0.0003, Max: 0.3761\n",
      "\n",
      "Alpha = 0.9\n",
      "  Test predictions - Mean: 0.0125, Min: -0.0002, Max: 0.3794\n",
      "\n",
      "Multi-Scale LP completed!\n"
     ]
    }
   ],
   "source": [
    "# Use full training data (no validation split for final submissions)\n",
    "y_train = y[train_idx].float()\n",
    "\n",
    "# Define alpha values for multi-scale propagation\n",
    "alphas = [0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "lp_predictions = {}\n",
    "\n",
    "print(\"Running Multi-Scale Label Propagation...\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Alpha = {alpha}\")\n",
    "    \n",
    "    # Label Propagation\n",
    "    lp = LabelPropagation(edge_index, num_nodes, num_layers=50, alpha=alpha, device=device)\n",
    "    y_lp = lp.propagate(y_train, train_idx)\n",
    "    \n",
    "    # Correct and Smooth\n",
    "    cs = CorrectAndSmooth(edge_index, num_nodes, num_layers=50,\n",
    "                          correction_alpha=0.8, smoothing_alpha=0.8, device=device)\n",
    "    y_corrected = cs.correct(y_lp, y_train, train_idx)\n",
    "    y_final = cs.smooth(y_corrected, y_train, train_idx)\n",
    "    \n",
    "    lp_predictions[alpha] = y_final.cpu()\n",
    "    \n",
    "    # Check statistics\n",
    "    test_preds = y_final[test_idx]\n",
    "    print(f\"  Test predictions - Mean: {test_preds.mean():.4f}, \"\n",
    "          f\"Min: {test_preds.min():.4f}, Max: {test_preds.max():.4f}\\n\")\n",
    "\n",
    "print(\"Multi-Scale LP completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0f65e",
   "metadata": {},
   "source": [
    "## 5. Link Prediction Features\n",
    "\n",
    "Compute link prediction scores for test nodes:\n",
    "- **Common Neighbors**: Count of shared neighbors\n",
    "- **Jaccard Coefficient**: Normalized overlap of neighborhoods\n",
    "- **Adamic-Adar**: Weighted common neighbors (by inverse log degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff2dc5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built adjacency list with 19765 nodes\n"
     ]
    }
   ],
   "source": [
    "# Move to CPU for efficient graph operations\n",
    "edge_index_cpu = edge_index.cpu()\n",
    "\n",
    "# Build adjacency list\n",
    "from collections import defaultdict\n",
    "\n",
    "adj_list = defaultdict(set)\n",
    "for i in range(edge_index_cpu.size(1)):\n",
    "    src, dst = edge_index_cpu[0, i].item(), edge_index_cpu[1, i].item()\n",
    "    adj_list[src].add(dst)\n",
    "\n",
    "print(f\"Built adjacency list with {len(adj_list)} nodes\")\n",
    "\n",
    "# Compute node degrees\n",
    "node_degrees = {node: len(neighbors) for node, neighbors in adj_list.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3df7b9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing link prediction features for test nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3365/3365 [02:12<00:00, 25.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Link Features Shape: (3365, 3)\n",
      "Common Neighbors - Mean: 1.11, Std: 1.57\n",
      "Jaccard - Mean: 0.0048, Std: 0.0037\n",
      "Adamic-Adar - Mean: 0.18, Std: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_link_features(node_idx, train_idx_cpu, adj_list, node_degrees):\n",
    "    \"\"\"\n",
    "    Compute link prediction features for a node based on training nodes.\n",
    "    Returns: common_neighbors, jaccard, adamic_adar scores\n",
    "    \"\"\"\n",
    "    neighbors = adj_list[node_idx]\n",
    "    \n",
    "    # Initialize scores\n",
    "    cn_scores = []\n",
    "    jaccard_scores = []\n",
    "    aa_scores = []\n",
    "    \n",
    "    # Compute scores with training nodes\n",
    "    for train_node in train_idx_cpu:\n",
    "        train_neighbors = adj_list[train_node]\n",
    "        \n",
    "        # Common Neighbors\n",
    "        common = neighbors.intersection(train_neighbors)\n",
    "        cn = len(common)\n",
    "        cn_scores.append(cn)\n",
    "        \n",
    "        # Jaccard Coefficient\n",
    "        union = neighbors.union(train_neighbors)\n",
    "        jaccard = cn / len(union) if len(union) > 0 else 0\n",
    "        jaccard_scores.append(jaccard)\n",
    "        \n",
    "        # Adamic-Adar\n",
    "        aa = sum(1.0 / np.log(node_degrees[z]) for z in common if node_degrees[z] > 1)\n",
    "        aa_scores.append(aa)\n",
    "    \n",
    "    return np.mean(cn_scores), np.mean(jaccard_scores), np.mean(aa_scores)\n",
    "\n",
    "# Compute link features for all test nodes\n",
    "train_idx_cpu = train_idx.cpu().tolist()\n",
    "test_idx_cpu = test_idx.cpu().tolist()\n",
    "\n",
    "link_features = np.zeros((len(test_idx_cpu), 3))  # CN, Jaccard, AA\n",
    "\n",
    "print(\"Computing link prediction features for test nodes...\")\n",
    "for i, node_idx in enumerate(tqdm(test_idx_cpu)):\n",
    "    cn, jaccard, aa = compute_link_features(node_idx, train_idx_cpu, adj_list, node_degrees)\n",
    "    link_features[i] = [cn, jaccard, aa]\n",
    "\n",
    "print(f\"\\nLink Features Shape: {link_features.shape}\")\n",
    "print(f\"Common Neighbors - Mean: {link_features[:, 0].mean():.2f}, Std: {link_features[:, 0].std():.2f}\")\n",
    "print(f\"Jaccard - Mean: {link_features[:, 1].mean():.4f}, Std: {link_features[:, 1].std():.4f}\")\n",
    "print(f\"Adamic-Adar - Mean: {link_features[:, 2].mean():.2f}, Std: {link_features[:, 2].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4c356",
   "metadata": {},
   "source": [
    "## 6. Convert Link Features to Label Predictions\n",
    "\n",
    "Use link features as similarity scores to weight training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2538185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating link-based predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3365/3365 [00:01<00:00, 2309.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link predictions shape: (3365, 305)\n",
      "Mean: 0.0406, Min: 0.0000, Max: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize link features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "link_features_norm = scaler.fit_transform(link_features)\n",
    "\n",
    "# Compute similarity-weighted label predictions\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "\n",
    "# For each test node, compute weighted average of training labels\n",
    "# based on link prediction scores\n",
    "link_predictions = []\n",
    "\n",
    "print(\"\\nGenerating link-based predictions...\")\n",
    "for i, test_node in enumerate(tqdm(test_idx_cpu)):\n",
    "    test_neighbors = adj_list[test_node]\n",
    "    \n",
    "    # Find training nodes that are neighbors\n",
    "    neighbor_train_mask = []\n",
    "    neighbor_train_indices = []\n",
    "    for j, train_node in enumerate(train_idx_cpu):\n",
    "        if train_node in test_neighbors:\n",
    "            neighbor_train_mask.append(True)\n",
    "            neighbor_train_indices.append(j)\n",
    "        else:\n",
    "            neighbor_train_mask.append(False)\n",
    "    \n",
    "    if len(neighbor_train_indices) > 0:\n",
    "        # Average labels of neighbor training nodes\n",
    "        neighbor_labels = y_train_np[neighbor_train_indices]\n",
    "        link_pred = neighbor_labels.mean(axis=0)\n",
    "    else:\n",
    "        # Use global average if no neighbors\n",
    "        link_pred = y_train_np.mean(axis=0)\n",
    "    \n",
    "    link_predictions.append(link_pred)\n",
    "\n",
    "link_predictions = np.array(link_predictions)\n",
    "print(f\"Link predictions shape: {link_predictions.shape}\")\n",
    "print(f\"Mean: {link_predictions.mean():.4f}, Min: {link_predictions.min():.4f}, Max: {link_predictions.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6e05b",
   "metadata": {},
   "source": [
    "## 7. Create Submission Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6850bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions, filename, test_idx):\n",
    "    \"\"\"\n",
    "    Create submission CSV with correct format.\n",
    "    \n",
    "    Args:\n",
    "        predictions: numpy array of shape (num_test_nodes, num_labels)\n",
    "        filename: output filename\n",
    "        test_idx: test node indices\n",
    "    \"\"\"\n",
    "    test_idx_cpu = test_idx.cpu().numpy()\n",
    "    \n",
    "    # CRITICAL: Clip predictions to [0, 1] range\n",
    "    predictions = np.clip(predictions, 0.0, 1.0)\n",
    "    \n",
    "    # Create DataFrame with correct column names: label_0, label_1, ..., label_304\n",
    "    num_labels = predictions.shape[1]\n",
    "    label_columns = [f'label_{i}' for i in range(num_labels)]\n",
    "    submission = pd.DataFrame(predictions, columns=label_columns)\n",
    "    submission.insert(0, 'node_id', test_idx_cpu)\n",
    "    \n",
    "    # Save\n",
    "    output_path = f'../Submissions/{filename}'\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved: {output_path}\")\n",
    "    print(f\"Shape: {submission.shape}\")\n",
    "    print(f\"Header: node_id,label_0,...,label_{num_labels-1}\")\n",
    "    print(f\"Mean prediction: {predictions.mean():.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b466bc",
   "metadata": {},
   "source": [
    "## 8. Generate Ensemble Submissions\n",
    "\n",
    "### Strategy:\n",
    "1. **Pure Multi-Scale LP**: Average of all 5 alpha values\n",
    "2. **LP 0.90 + 5% Link**: 95% best LP (α=0.9) + 5% link features\n",
    "3. **LP 0.90 + 10% Link**: 90% best LP (α=0.9) + 10% link features\n",
    "4. **Multi-Scale + 5% Link**: 95% multi-scale average + 5% link\n",
    "5. **Adaptive Ensemble**: Use α=0.90 for common labels, α=0.75 for rare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d52e21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions extracted for all LP alphas\n"
     ]
    }
   ],
   "source": [
    "# Get test predictions for each method\n",
    "test_idx_cpu = test_idx.cpu()\n",
    "\n",
    "lp_test_preds = {}\n",
    "for alpha in alphas:\n",
    "    lp_test_preds[alpha] = lp_predictions[alpha][test_idx_cpu].numpy()\n",
    "\n",
    "print(\"Test predictions extracted for all LP alphas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5f965",
   "metadata": {},
   "source": [
    "### Submission 1: Pure Multi-Scale LP Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e859bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Submissions/submission_Draft11_MultiScale_Avg.csv\n",
      "Shape: (3365, 306)\n",
      "Header: node_id,label_0,...,label_304\n",
      "Mean prediction: 0.011965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average all 5 LP predictions\n",
    "multi_scale_avg = np.mean([lp_test_preds[alpha] for alpha in alphas], axis=0)\n",
    "\n",
    "create_submission(\n",
    "    multi_scale_avg,\n",
    "    'submission_Draft11_MultiScale_Avg.csv',\n",
    "    test_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb74ec",
   "metadata": {},
   "source": [
    "### Submission 2: LP 0.90 + 5% Link Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cc1dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Submissions/submission_Draft11_LP90_Link5.csv\n",
      "Shape: (3365, 306)\n",
      "Header: node_id,label_0,...,label_304\n",
      "Mean prediction: 0.013877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 95% LP (alpha=0.9) + 5% link predictions\n",
    "ensemble_95_5 = 0.95 * lp_test_preds[0.90] + 0.05 * link_predictions\n",
    "\n",
    "create_submission(\n",
    "    ensemble_95_5,\n",
    "    'submission_Draft11_LP90_Link5.csv',\n",
    "    test_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a8c60",
   "metadata": {},
   "source": [
    "### Submission 3: LP 0.90 + 10% Link Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5d39aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Submissions/submission_Draft11_LP90_Link10.csv\n",
      "Shape: (3365, 306)\n",
      "Header: node_id,label_0,...,label_304\n",
      "Mean prediction: 0.015285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 90% LP (alpha=0.9) + 10% link predictions\n",
    "ensemble_90_10 = 0.90 * lp_test_preds[0.90] + 0.10 * link_predictions\n",
    "\n",
    "create_submission(\n",
    "    ensemble_90_10,\n",
    "    'submission_Draft11_LP90_Link10.csv',\n",
    "    test_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ea6da",
   "metadata": {},
   "source": [
    "### Submission 4: Multi-Scale Average + 5% Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53327bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../Submissions/submission_Draft11_MultiScale_Link5.csv\n",
      "Shape: (3365, 306)\n",
      "Header: node_id,label_0,...,label_304\n",
      "Mean prediction: 0.013399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 95% multi-scale average + 5% link predictions\n",
    "ensemble_multi_link5 = 0.95 * multi_scale_avg + 0.05 * link_predictions\n",
    "\n",
    "create_submission(\n",
    "    ensemble_multi_link5,\n",
    "    'submission_Draft11_MultiScale_Link5.csv',\n",
    "    test_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1c079",
   "metadata": {},
   "source": [
    "### Submission 5: Adaptive Multi-Scale by Label Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9990c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare labels: 37\n",
      "Medium labels: 184\n",
      "Common labels: 84\n",
      "Saved: ../Submissions/submission_Draft11_Adaptive.csv\n",
      "Shape: (3365, 306)\n",
      "Header: node_id,label_0,...,label_304\n",
      "Mean prediction: 0.012340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute label frequencies\n",
    "label_freq = y_train.sum(dim=0).cpu().numpy()\n",
    "\n",
    "# Categorize labels\n",
    "rare_labels = label_freq < 50\n",
    "medium_labels = (label_freq >= 50) & (label_freq < 200)\n",
    "common_labels = label_freq >= 200\n",
    "\n",
    "print(f\"Rare labels: {rare_labels.sum()}\")\n",
    "print(f\"Medium labels: {medium_labels.sum()}\")\n",
    "print(f\"Common labels: {common_labels.sum()}\")\n",
    "\n",
    "# Adaptive ensemble\n",
    "adaptive_ensemble = np.zeros_like(multi_scale_avg)\n",
    "\n",
    "# Rare labels: use lower alpha (0.75) for more smoothing\n",
    "adaptive_ensemble[:, rare_labels] = lp_test_preds[0.75][:, rare_labels]\n",
    "\n",
    "# Medium labels: use medium alpha (0.85)\n",
    "adaptive_ensemble[:, medium_labels] = lp_test_preds[0.85][:, medium_labels]\n",
    "\n",
    "# Common labels: use high alpha (0.90) for less smoothing\n",
    "adaptive_ensemble[:, common_labels] = lp_test_preds[0.90][:, common_labels]\n",
    "\n",
    "create_submission(\n",
    "    adaptive_ensemble,\n",
    "    'submission_Draft11_Adaptive.csv',\n",
    "    test_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15188155",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Generated 5 submissions:\n",
    "\n",
    "1. **submission_Draft11_MultiScale_Avg.csv**: Average of 5 LP alphas (0.70-0.90)\n",
    "2. **submission_Draft11_LP90_Link5.csv**: 95% LP (α=0.9) + 5% link features\n",
    "3. **submission_Draft11_LP90_Link10.csv**: 90% LP (α=0.9) + 10% link features\n",
    "4. **submission_Draft11_MultiScale_Link5.csv**: 95% multi-scale + 5% link\n",
    "5. **submission_Draft11_Adaptive.csv**: Adaptive alpha by label frequency\n",
    "\n",
    "**Next Steps:**\n",
    "- Submit all 5 files to Kaggle\n",
    "- If any improves over 0.0571, proceed to **Option B** (riskier features)\n",
    "- Track which strategies work best for final submissions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv - GNN Project)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
