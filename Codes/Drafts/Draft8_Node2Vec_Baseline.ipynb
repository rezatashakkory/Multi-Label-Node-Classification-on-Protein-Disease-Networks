{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9042a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "# FORCE FULL CPU UTILIZATION\n",
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"20\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c76aee",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b9da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "Nodes: 19,765\n",
      "Edges: 1,554,790\n",
      "Labels: 305\n",
      "Train: 5,046 | Test: 3,365\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/'\n",
    "\n",
    "edge_index = torch.load(data_dir + 'edge_index.pt')\n",
    "node_features = torch.load(data_dir + 'node_features.pt')\n",
    "y = torch.load(data_dir + 'y.pt')\n",
    "train_idx = torch.load(data_dir + 'train_idx.pt')\n",
    "test_idx = torch.load(data_dir + 'test_idx.pt')\n",
    "\n",
    "num_nodes = node_features.shape[0]\n",
    "num_edges = edge_index.shape[1]\n",
    "num_labels = y.shape[1]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Nodes: {num_nodes:,}\")\n",
    "print(f\"Edges: {num_edges:,}\")\n",
    "print(f\"Labels: {num_labels}\")\n",
    "print(f\"Train: {len(train_idx):,} | Test: {len(test_idx):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aca700",
   "metadata": {},
   "source": [
    "## 2. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb4c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4036 | Val: 1010 | Test: 3365\n"
     ]
    }
   ],
   "source": [
    "# 80/20 split\n",
    "train_size = int(0.8 * len(train_idx))\n",
    "perm = torch.randperm(len(train_idx))\n",
    "train_indices = train_idx[perm[:train_size]]\n",
    "val_indices = train_idx[perm[train_size:]]\n",
    "\n",
    "# Create masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "print(f\"Train: {train_mask.sum()} | Val: {val_mask.sum()} | Test: {test_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd79ac",
   "metadata": {},
   "source": [
    "## 3. Train Node2Vec Embeddings\n",
    "\n",
    "**Configuration:**\n",
    "- Embedding dims: 64, 128 (test both)\n",
    "- Walk length: 20\n",
    "- Context size: 10\n",
    "- Walks per node: 10\n",
    "- p=1, q=1 (balanced exploration)\n",
    "\n",
    "**Note:** Using num_workers=0 for Windows compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35850e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node2vec(edge_index, num_nodes, embedding_dim=64, epochs=100):\n",
    "    \"\"\"\n",
    "    Train Node2Vec embeddings.\n",
    "    \n",
    "    Args:\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        num_nodes: Number of nodes\n",
    "        embedding_dim: Embedding dimension\n",
    "        epochs: Training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Embeddings tensor [num_nodes, embedding_dim]\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING NODE2VEC ({embedding_dim}D)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize Node2Vec\n",
    "    model = Node2Vec(\n",
    "        edge_index=edge_index,\n",
    "        embedding_dim=embedding_dim,\n",
    "        walk_length=20,\n",
    "        context_size=10,\n",
    "        walks_per_node=10,\n",
    "        p=1.0,  # Return parameter\n",
    "        q=1.0,  # In-out parameter\n",
    "        num_negative_samples=1,\n",
    "        sparse=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create data loader (num_workers=0 for Windows)\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Optimizer (sparse optimizer for sparse embeddings)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1), desc=f\"Node2Vec {embedding_dim}d\"):\n",
    "        epoch_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        total_loss += epoch_loss / len(loader)\n",
    "        \n",
    "        if epoch % 20 == 0 or epoch == 1:\n",
    "            avg_loss = epoch_loss / len(loader)\n",
    "            print(f\"  Epoch {epoch:3d} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(torch.arange(num_nodes, device=device))\n",
    "    \n",
    "    print(f\"\\n‚úì Node2Vec {embedding_dim}d training complete!\")\n",
    "    print(f\"  Final loss: {total_loss / epochs:.4f}\")\n",
    "    print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17b2eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING NODE2VEC (64D)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d62d2ad4f22460680e9037050ea5acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Node2Vec 64d:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 | Loss: 4.2412\n",
      "  Epoch  20 | Loss: 1.0898\n",
      "  Epoch  40 | Loss: 1.0871\n",
      "  Epoch  60 | Loss: 1.0868\n",
      "  Epoch  80 | Loss: 1.0865\n",
      "  Epoch 100 | Loss: 1.0855\n",
      "\n",
      "‚úì Node2Vec 64d training complete!\n",
      "  Final loss: 1.1306\n",
      "  Embeddings shape: torch.Size([19765, 64])\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING NODE2VEC (128D)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26146462a8a34ceab22fceac396e4c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Node2Vec 128d:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 | Loss: 6.7013\n",
      "  Epoch  20 | Loss: 1.0912\n",
      "  Epoch  40 | Loss: 1.0892\n",
      "  Epoch  60 | Loss: 1.0886\n",
      "  Epoch  80 | Loss: 1.0881\n",
      "  Epoch 100 | Loss: 1.0871\n",
      "\n",
      "‚úì Node2Vec 128d training complete!\n",
      "  Final loss: 1.1760\n",
      "  Embeddings shape: torch.Size([19765, 128])\n",
      "================================================================================\n",
      "\n",
      "‚úì Embeddings saved for reuse in Draft8_GAT_Enhanced\n"
     ]
    }
   ],
   "source": [
    "# Train both 64d and 128d embeddings\n",
    "embeddings_64d = train_node2vec(edge_index, num_nodes, embedding_dim=64, epochs=100)\n",
    "embeddings_128d = train_node2vec(edge_index, num_nodes, embedding_dim=128, epochs=100)\n",
    "\n",
    "# Save for reuse\n",
    "torch.save(embeddings_64d, '../data/node2vec_64d_draft8.pt')\n",
    "torch.save(embeddings_128d, '../data/node2vec_128d_draft8.pt')\n",
    "print(\"\\n‚úì Embeddings saved for reuse in Draft8_GAT_Enhanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780728b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab80158",
   "metadata": {},
   "source": [
    "### After this, proceed to train simple MLP classifier on the embeddings;\n",
    "### Resutls, were not satisfactory, so further tuning or alternative methods may be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af83a94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0e1c9",
   "metadata": {},
   "source": [
    "## 4. Simple MLP Classifier\n",
    "\n",
    "**Architecture:**\n",
    "- Input: Node2Vec embeddings (64d or 128d)\n",
    "- Hidden: 256 ‚Üí 256 (2 layers)\n",
    "- Output: 305 disease labels\n",
    "- Activation: ReLU\n",
    "- Dropout: 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fa9302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SimpleMLP defined\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 2-layer MLP for structure-only baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úì SimpleMLP defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8351edc",
   "metadata": {},
   "source": [
    "## 5. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7a2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ap(y_true, y_pred, mask):\n",
    "    \"\"\"Compute micro-averaged Average Precision.\"\"\"\n",
    "    y_true_np = y_true[mask].cpu().numpy().ravel()\n",
    "    y_pred_np = y_pred[mask].cpu().detach().numpy().ravel()\n",
    "    return average_precision_score(y_true_np, y_pred_np, average='micro')\n",
    "\n",
    "\n",
    "def train_mlp(embeddings, y, train_mask, val_mask, embedding_name=\"64d\", epochs=200):\n",
    "    \"\"\"\n",
    "    Train MLP on Node2Vec embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Node2Vec embeddings [num_nodes, embedding_dim]\n",
    "        y: Labels [num_nodes, num_labels]\n",
    "        train_mask: Training node mask\n",
    "        val_mask: Validation node mask\n",
    "        embedding_name: Name for logging\n",
    "        epochs: Training epochs\n",
    "    \n",
    "    Returns:\n",
    "        trained model, best_val_ap, final predictions\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING MLP ON NODE2VEC {embedding_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Move to device\n",
    "    x = embeddings.to(device)\n",
    "    y_device = y.to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    in_dim = embeddings.shape[1]\n",
    "    hidden_dim = 256\n",
    "    out_dim = num_labels\n",
    "    \n",
    "    model = SimpleMLP(in_dim, hidden_dim, out_dim, dropout=0.3).to(device)\n",
    "    \n",
    "    print(f\"\\nModel: SimpleMLP\")\n",
    "    print(f\"  Input dim: {in_dim}\")\n",
    "    print(f\"  Hidden dim: {hidden_dim}\")\n",
    "    print(f\"  Output dim: {out_dim}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Compute class weights (same as Draft5)\n",
    "    pos_count = y[train_mask].sum(dim=0).float()\n",
    "    neg_count = train_mask.sum() - pos_count\n",
    "    pos_weight = (neg_count / pos_count.clamp(min=1)).clamp(max=50).to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=10\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_ap = 0\n",
    "    best_state = None\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nTraining for up to {epochs} epochs...\\n\")\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1), desc=f\"Training {embedding_name}\"):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(x)\n",
    "        loss = criterion(out[train_mask], y_device[train_mask].float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(x)\n",
    "                probs = torch.sigmoid(out)\n",
    "                \n",
    "                train_ap = evaluate_ap(y_device, probs, train_mask)\n",
    "                val_ap = evaluate_ap(y_device, probs, val_mask)\n",
    "            \n",
    "            scheduler.step(val_ap)\n",
    "            \n",
    "            if epoch % 20 == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | \"\n",
    "                      f\"Train AP: {train_ap:.4f} | Val AP: {val_ap:.4f}\")\n",
    "            \n",
    "            # Save best\n",
    "            if val_ap > best_val_ap:\n",
    "                best_val_ap = val_ap\n",
    "                best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    \n",
    "    # Final predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_out = model(x)\n",
    "        final_probs = torch.sigmoid(final_out)\n",
    "    \n",
    "    print(f\"\\n‚úì MLP training complete!\")\n",
    "    print(f\"‚úì Best validation AP: {best_val_ap:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return model, best_val_ap, final_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c65db5",
   "metadata": {},
   "source": [
    "## 6. Train MLPs on Both Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d791f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING MLP ON NODE2VEC 64d\n",
      "================================================================================\n",
      "\n",
      "Model: SimpleMLP\n",
      "  Input dim: 64\n",
      "  Hidden dim: 256\n",
      "  Output dim: 305\n",
      "  Parameters: 161,841\n",
      "\n",
      "Training for up to 200 epochs...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd5493b8ad140abb08244657dcea6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 64d:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 1.3090 | Train AP: 0.0359 | Val AP: 0.0341\n",
      "Epoch  20 | Loss: 1.0408 | Train AP: 0.1143 | Val AP: 0.0782\n",
      "Epoch  40 | Loss: 0.9174 | Train AP: 0.1905 | Val AP: 0.0816\n",
      "Epoch  60 | Loss: 0.8344 | Train AP: 0.2732 | Val AP: 0.0840\n",
      "Epoch  80 | Loss: 0.7828 | Train AP: 0.3214 | Val AP: 0.0853\n",
      "Epoch 100 | Loss: 0.7475 | Train AP: 0.3603 | Val AP: 0.0844\n",
      "Epoch 120 | Loss: 0.7196 | Train AP: 0.3859 | Val AP: 0.0829\n",
      "Epoch 140 | Loss: 0.6873 | Train AP: 0.4105 | Val AP: 0.0839\n",
      "Epoch 160 | Loss: 0.6700 | Train AP: 0.4313 | Val AP: 0.0832\n",
      "Epoch 180 | Loss: 0.6514 | Train AP: 0.4522 | Val AP: 0.0844\n",
      "Epoch 200 | Loss: 0.6433 | Train AP: 0.4698 | Val AP: 0.0845\n",
      "\n",
      "‚úì MLP training complete!\n",
      "‚úì Best validation AP: 0.0854\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING MLP ON NODE2VEC 128d\n",
      "================================================================================\n",
      "\n",
      "Model: SimpleMLP\n",
      "  Input dim: 128\n",
      "  Hidden dim: 256\n",
      "  Output dim: 305\n",
      "  Parameters: 178,225\n",
      "\n",
      "Training for up to 200 epochs...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fa7da20f0940e78ab41476e1ec73c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 128d:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Loss: 1.2977 | Train AP: 0.0378 | Val AP: 0.0353\n",
      "Epoch  20 | Loss: 1.0002 | Train AP: 0.1138 | Val AP: 0.0676\n",
      "Epoch  40 | Loss: 0.8215 | Train AP: 0.2687 | Val AP: 0.0742\n",
      "Epoch  60 | Loss: 0.7247 | Train AP: 0.3698 | Val AP: 0.0736\n",
      "Epoch  80 | Loss: 0.6664 | Train AP: 0.4297 | Val AP: 0.0722\n",
      "Epoch 100 | Loss: 0.6339 | Train AP: 0.4760 | Val AP: 0.0728\n",
      "Epoch 120 | Loss: 0.5946 | Train AP: 0.5064 | Val AP: 0.0725\n",
      "Epoch 140 | Loss: 0.5743 | Train AP: 0.5279 | Val AP: 0.0722\n",
      "Epoch 160 | Loss: 0.5560 | Train AP: 0.5536 | Val AP: 0.0748\n",
      "Epoch 180 | Loss: 0.5393 | Train AP: 0.5745 | Val AP: 0.0710\n",
      "Epoch 200 | Loss: 0.5236 | Train AP: 0.5882 | Val AP: 0.0698\n",
      "\n",
      "‚úì MLP training complete!\n",
      "‚úì Best validation AP: 0.0758\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train on 64d embeddings\n",
    "mlp_64d, val_ap_64d, probs_64d = train_mlp(\n",
    "    embeddings_64d, y, train_mask, val_mask, \n",
    "    embedding_name=\"64d\", epochs=200\n",
    ")\n",
    "\n",
    "# Train on 128d embeddings\n",
    "mlp_128d, val_ap_128d, probs_128d = train_mlp(\n",
    "    embeddings_128d, y, train_mask, val_mask,\n",
    "    embedding_name=\"128d\", epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10e204",
   "metadata": {},
   "source": [
    "## 7. Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e0d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRUCTURE-ONLY BASELINE RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä Validation Micro-AP:\n",
      "  Node2Vec 64d + MLP:  0.0854\n",
      "  Node2Vec 128d + MLP: 0.0758\n",
      "\n",
      "üìà Comparison to Existing Methods (from previous drafts):\n",
      "  GAT (Draft4):        ~0.051 (Kaggle)\n",
      "  LP+C&S (Draft5):     ~0.056 (Kaggle)\n",
      "  Best on leaderboard: ~0.064\n",
      "\n",
      "üèÜ BEST: Node2Vec 64d + MLP ‚Üí 0.0854\n",
      "\n",
      "üí° INTERPRETATION:\n",
      "  ‚úì Structure-only baseline is STRONG (‚â•0.055)\n",
      "  ‚Üí Graph structure carries most of the signal\n",
      "  ‚Üí Focus on methods that exploit structure:\n",
      "    ‚Ä¢ GNNs with structural embeddings\n",
      "    ‚Ä¢ Concatenate Node2Vec + bio features in GAT\n",
      "    ‚Ä¢ Apply C&S post-processing\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRUCTURE-ONLY BASELINE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Validation Micro-AP:\")\n",
    "print(f\"  Node2Vec 64d + MLP:  {val_ap_64d:.4f}\")\n",
    "print(f\"  Node2Vec 128d + MLP: {val_ap_128d:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Comparison to Existing Methods (from previous drafts):\")\n",
    "print(f\"  GAT (Draft4):        ~0.051 (Kaggle)\")\n",
    "print(f\"  LP+C&S (Draft5):     ~0.056 (Kaggle)\")\n",
    "print(f\"  Best on leaderboard: ~0.064\")\n",
    "\n",
    "# Determine best embedding\n",
    "if val_ap_128d > val_ap_64d:\n",
    "    best_embedding_dim = 128\n",
    "    best_val_ap = val_ap_128d\n",
    "    best_probs = probs_128d\n",
    "    best_embeddings = embeddings_128d\n",
    "else:\n",
    "    best_embedding_dim = 64\n",
    "    best_val_ap = val_ap_64d\n",
    "    best_probs = probs_64d\n",
    "    best_embeddings = embeddings_64d\n",
    "\n",
    "print(f\"\\nüèÜ BEST: Node2Vec {best_embedding_dim}d + MLP ‚Üí {best_val_ap:.4f}\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION:\")\n",
    "if best_val_ap >= 0.055:\n",
    "    print(\"  ‚úì Structure-only baseline is STRONG (‚â•0.055)\")\n",
    "    print(\"  ‚Üí Graph structure carries most of the signal\")\n",
    "    print(\"  ‚Üí Focus on methods that exploit structure:\")\n",
    "    print(\"    ‚Ä¢ GNNs with structural embeddings\")\n",
    "    print(\"    ‚Ä¢ Concatenate Node2Vec + bio features in GAT\")\n",
    "    print(\"    ‚Ä¢ Apply C&S post-processing\")\n",
    "elif best_val_ap >= 0.045:\n",
    "    print(\"  ‚úì Structure-only baseline is MODERATE (0.045-0.055)\")\n",
    "    print(\"  ‚Üí Structure + features both important\")\n",
    "    print(\"  ‚Üí Recommended approach:\")\n",
    "    print(\"    ‚Ä¢ GAT with [bio features || Node2Vec]\")\n",
    "    print(\"    ‚Ä¢ Keep LP+C&S as strong baseline\")\n",
    "else:\n",
    "    print(\"  ‚úì Structure-only baseline is WEAK (<0.045)\")\n",
    "    print(\"  ‚Üí LP+C&S is the main hero\")\n",
    "    print(\"  ‚Üí Focus on label smoothing:\")\n",
    "    print(\"    ‚Ä¢ Optimize C&S hyperparameters\")\n",
    "    print(\"    ‚Ä¢ Try label reuse tricks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7880d",
   "metadata": {},
   "source": [
    "## 8. Generate Test Predictions (If Good Enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d54d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING SUBMISSION (Val AP ‚â• 0.053)\n",
      "================================================================================\n",
      "\n",
      "‚úì Submission saved: submission_Draft8_Node2Vec_MLP.csv\n",
      "  Embedding: Node2Vec 64d\n",
      "  Validation AP: 0.0854\n",
      "  Mean prediction: 0.3111\n",
      "\n",
      "üí° Consider submitting if:\n",
      "  ‚Ä¢ Val AP > your current best (0.0854)\n",
      "  ‚Ä¢ Mean prediction looks reasonable (~0.02-0.03)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Only generate submission if validation AP suggests it's competitive\n",
    "SUBMIT_THRESHOLD = 0.053  # Only if better than current GAT\n",
    "\n",
    "if best_val_ap >= SUBMIT_THRESHOLD:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING SUBMISSION (Val AP ‚â• 0.053)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Test predictions\n",
    "    test_probs = best_probs[test_mask].cpu().numpy()\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame(test_probs)\n",
    "    submission.to_csv('../Submissions/submission_Draft8_Node2Vec_MLP.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Submission saved: submission_Draft8_Node2Vec_MLP.csv\")\n",
    "    print(f\"  Embedding: Node2Vec {best_embedding_dim}d\")\n",
    "    print(f\"  Validation AP: {best_val_ap:.4f}\")\n",
    "    print(f\"  Mean prediction: {test_probs.mean():.4f}\")\n",
    "    print(f\"\\nüí° Consider submitting if:\")\n",
    "    print(f\"  ‚Ä¢ Val AP > your current best ({best_val_ap:.4f})\")\n",
    "    print(f\"  ‚Ä¢ Mean prediction looks reasonable (~0.02-0.03)\")\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"NOT GENERATING SUBMISSION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Validation AP: {best_val_ap:.4f} < {SUBMIT_THRESHOLD:.3f}\")\n",
    "    print(f\"  ‚Üí Structure-only is weaker than GAT\")\n",
    "    print(f\"  ‚Üí But we'll use Node2Vec embeddings in Draft8_GAT_Enhanced!\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3397210",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d831002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DRAFT8 NODE2VEC BASELINE - SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ COMPLETED:\n",
      "  1. Trained Node2Vec embeddings (64d, 128d)\n",
      "  2. Trained SimpleMLP on structure-only\n",
      "  3. Evaluated validation micro-AP\n",
      "  4. Best result: 0.0854 (Node2Vec 64d)\n",
      "\n",
      "üìÅ FILES SAVED:\n",
      "  ‚Ä¢ node2vec_64d_draft8.pt\n",
      "  ‚Ä¢ node2vec_128d_draft8.pt\n",
      "  ‚Ä¢ submission_Draft8_Node2Vec_MLP.csv\n",
      "\n",
      "‚û°Ô∏è NEXT STEPS:\n",
      "  1. Open Draft8_GAT_Enhanced.ipynb\n",
      "  2. Test GAT with 3 input configurations:\n",
      "     a) Bio features + log-degree (baseline)\n",
      "     b) Node2Vec only\n",
      "     c) [Bio features || log-degree || Node2Vec] (recommended)\n",
      "  3. Pick best variant on validation\n",
      "  4. Apply C&S in Draft8_BestModel_CS.ipynb\n",
      "\n",
      "================================================================================\n",
      "Ready to proceed to Draft8_GAT_Enhanced! üöÄ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DRAFT8 NODE2VEC BASELINE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ COMPLETED:\")\n",
    "print(\"  1. Trained Node2Vec embeddings (64d, 128d)\")\n",
    "print(\"  2. Trained SimpleMLP on structure-only\")\n",
    "print(\"  3. Evaluated validation micro-AP\")\n",
    "print(f\"  4. Best result: {best_val_ap:.4f} (Node2Vec {best_embedding_dim}d)\")\n",
    "\n",
    "print(\"\\nüìÅ FILES SAVED:\")\n",
    "print(\"  ‚Ä¢ node2vec_64d_draft8.pt\")\n",
    "print(\"  ‚Ä¢ node2vec_128d_draft8.pt\")\n",
    "if best_val_ap >= SUBMIT_THRESHOLD:\n",
    "    print(\"  ‚Ä¢ submission_Draft8_Node2Vec_MLP.csv\")\n",
    "\n",
    "print(\"\\n‚û°Ô∏è NEXT STEPS:\")\n",
    "print(\"  1. Open Draft8_GAT_Enhanced.ipynb\")\n",
    "print(\"  2. Test GAT with 3 input configurations:\")\n",
    "print(\"     a) Bio features + log-degree (baseline)\")\n",
    "print(\"     b) Node2Vec only\")\n",
    "print(\"     c) [Bio features || log-degree || Node2Vec] (recommended)\")\n",
    "print(\"  3. Pick best variant on validation\")\n",
    "print(\"  4. Apply C&S in Draft8_BestModel_CS.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to proceed to Draft8_GAT_Enhanced! üöÄ\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv - GNN Project)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
